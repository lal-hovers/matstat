
>  Билет: Независимость двух случайных величин. Пример зависимых с.в. с формально факторизованной совместной плотностью. Нулевая корреляция и независимость двух случайных величин, взаимосвязь (с доказательством). Пример зависимых с.в. с нулевой корреляцией. Среднее значение произведения независимых с.в. (доказательство для двумерной с.в.)


## 1. Определение независимости двух случайных величин

Пусть имеются две случайные величины $X$ и $Y$ (дискретные или непрерывные).

1. **Дискретный случай.**  
   Говорят, что $X$ и $Y$ **независимы**, если для любых значений $x_i$ и $y_j$:
   $$
   P\bigl(X = x_i,\; Y = y_j\bigr) 
   \;=\; P\bigl(X = x_i\bigr)\, P\bigl(Y = y_j\bigr).
   $$
   Эквивалентно: их совместная функция вероятностей факторизуется как
   $$
   p_{X,Y}(x_i, y_j)
   \;=\; p_X(x_i)\; p_Y(y_j).
   $$

2. **Непрерывный случай.**  
   Говорят, что $X$ и $Y$ **независимы**, если для любых борелевских множеств $A, B \subset \mathbb{R}$ выполняется
   $$
   P(X \in A,\; Y \in B)
   \;=\; P(X \in A)\, P(Y \in B).
   $$
   Если у $(X, Y)$ есть совместная плотность $f_{X,Y}(x,y)$ и у $X$, $Y$ есть маргинальные плотности $f_X(x)$ и $f_Y(y)$, то независимость эквивалентна факторизации:
   $$
   f_{X,Y}(x,y)
   \;=\; f_X(x)\; f_Y(y)
   \quad \text{(для почти всех $(x,y)$).}
   $$

**Интуитивно**: $X$ и $Y$ независимы, если «знание» одного из них не даёт информации о другом.  

---

## 2. Пример зависимости случайных величин при формально факторизованной плотности

Иногда формальные записи могут вводить в заблуждение, если их неправильно интерпретировать. Однако **по определению** независимости нужно требовать равенство $f_{X,Y}(x,y) = f_X(x)\,f_Y(y)$ (или $p_{X,Y}(x_i,y_j) = p_X(x_i)\,p_Y(y_j)$ в дискретном случае) **для всех** $(x,y)$. Если где-то это равенство не выполняется, значит, зависимость присутствует.

**Пример** (в непрерывном случае). Предположим, что мы «пытались» записать
$$
f_{X,Y}(x,y) = \begin{cases}
1, & x \in (0,1),\, y = x,\\
0, & \text{иначе}.
\end{cases}
$$
Здесь как будто можно было бы найти $f_X(x)$ и $f_Y(y)$ отдельно, но на самом деле такая «плотность» не является функцией двух переменных в обычном смысле (распределение сосредоточено на прямой $y=x$). В этом случае $X$ и $Y$ фактически жёстко связаны (зависимы: $Y=X$ почти всюду), а не независимы.  

---

## 3. Нулевая корреляция и независимость

### 3.1. [[Корреляция]] 

**Корреляция** (Пирсона) между $X$ и $Y$ определяется формулой
$$
\rho_{X,Y} 
= \frac{\mathrm{Cov}(X,Y)}{\sqrt{\mathrm{Var}(X)}\,\sqrt{\mathrm{Var}(Y)}},
$$
где
$$
\mathrm{Cov}(X,Y) 
= E\bigl[(X - E(X))(Y - E(Y))\bigr]
= E(XY) - E(X)\,E(Y).
$$

Если $\rho_{X,Y} = 0$, говорят, что $X$ и $Y$ **некоррелированы**. Однако **некоррелированность** (нулевая корреляция) **не** эквивалентна независимости.

### 3.2. Пример зависимых случайных величин с нулевой корреляцией

Рассмотрим случайную величину $X$, равномерно распределённую на отрезке $(-1,1)$, и определим
$$
Y = X^2.
$$
- Очевидно, $Y$ однозначно выражается через $X$ (значит, $X$ и $Y$ зависимы).  
- Вычислим их корреляцию:

1. $E(X) = 0$ (симметричное распределение вокруг 0).  
2. $E(Y) = E(X^2) = \frac{1}{1 - (-1)} \int_{-1}^{1} x^2 \, \frac{dx}{2}$ или просто известный результат для $\mathrm{Uniform}(-1,1)$:  
   $$
   E(X^2) = \frac{(1-(-1))^3}{12\cdot(1-(-1))} \quad\text{(есть стандартная формула)} 
   = \frac{1}{3}.
   $$
   Но проще: $E(X^2)$ для $\mathrm{Uniform}(-1,1)$ равно $\tfrac{1}{3}$.  

3. $E(XY) = E\bigl(X \cdot X^2\bigr) = E(X^3)$.  
   Поскольку $X$ симметрична, $X^3$ тоже симметрична с равными положительными и отрицательными площадями. Итого $E(X^3) = 0$.  

Следовательно,
$$
\mathrm{Cov}(X, Y) 
= E(XY) - E(X)\,E(Y)
= 0 - 0\cdot \tfrac13
= 0.
$$
Значит, $\rho_{X,Y} = 0$. Но $Y = X^2$ полностью определяется $X$ (то есть зависимы). Это классический пример нулевой корреляции без независимости.

---

## 4. Среднее значение произведения независимых случайных величин

**Теорема**: Если $X$ и $Y$ **независимы**, то
$$
E(XY) = E(X)\,E(Y).
$$

### 4.1. Доказательство (дискретный случай)

Пусть $X$ и $Y$ — независимы. Тогда
$$
p_{X,Y}(x_i, y_j) = p_X(x_i)\, p_Y(y_j).
$$
Тогда
$$
E(XY)
= \sum_{i,j} x_i \, y_j \, p_{X,Y}(x_i, y_j)
= \sum_{i,j} x_i \, y_j \, p_X(x_i)\, p_Y(y_j).
$$
Рассмотрим отдельно суммы:
$$
\sum_{i} x_i \, p_X(x_i) = E(X), 
\quad
\sum_{j} y_j \, p_Y(y_j) = E(Y).
$$
Заметим, что
$$
\sum_{i,j} x_i \, y_j \, p_X(x_i)\, p_Y(y_j)
= \Bigl(\sum_{i} x_i \, p_X(x_i)\Bigr)
\Bigl(\sum_{j} y_j \, p_Y(y_j)\Bigr)
= E(X)\, E(Y).
$$
Таким образом,
$$
E(XY) = E(X)\,E(Y).
$$

### 4.2. Доказательство (непрерывный случай)

Аналогично, если $X$ и $Y$ независимы, то
$$
f_{X,Y}(x,y) = f_X(x)\, f_Y(y).
$$
Тогда
$$
E(XY)
= \iint x\, y \, f_{X,Y}(x,y)\, dx\, dy
= \iint x\, y \, f_X(x)\, f_Y(y)\, dx\, dy.
$$
Разделяем интеграл:
$$
= \left(\int x \, f_X(x)\, dx\right)\,\left(\int y \, f_Y(y)\, dy\right)
= E(X)\, E(Y).
$$

---

## 5. Итог

1. **Независимость** двух случайных величин $X$ и $Y$ означает факторизацию их совместной вероятностной меры (дискретно) или плотности (непрерывно).  
2. **Корреляция** равна нулю не всегда означает независимость. Пример: $X \sim \mathrm{Uniform}(-1,1)$, $Y=X^2$. Они зависят, но имеют нулевую корреляцию.  
3. **Среднее значение произведения независимых с.в.** равно произведению средних:  
   $$
   E(XY) = E(X)\, E(Y).
   $$
   Это напрямую вытекает из факторизации вероятностей или плотностей при независимости.

Таким образом, ключевые моменты билета:
- Независимость требует равенства $p_{X,Y}(x_i,y_j) = p_X(x_i)\,p_Y(y_j)$ или $f_{X,Y}(x,y) = f_X(x)\,f_Y(y)$.
- Нулевая корреляция ($\rho=0$) не гарантирует независимости.
- У независимых величин $E(XY) = E(X)\,E(Y)$.

---
linkes:
---
# Биномиальное распределение и его основные параметры. Доказать, что если $X \sim \text{Bin}(n, p)$, то $E(X) = np$.

## **TL;Dr:**

1. Если $X \sim \text{Bin}(n, p)$, то $X$ — это число успехов в $n$ независимых испытаниях, в каждом из которых успех наступает с вероятностью $p$.
2. Основные параметры биномиального распределения:
   - $E(X) = np$,
   - $\mathrm{Var}(X) = np(1 - p)$.
3. Формула для математического ожидания $E(X) = np$ доказывается либо через сумму индикаторных (бернуллиевских) случайных величин, либо с помощью комбинаторных преобразований в сумме $\sum k \binom{n}{k} p^k (1-p)^{n-k}$.


## 1. Определение биномиального распределения

> [[Биномиальное распределение|Биномиальное распределение]] **с параметрами $n$ и $p$ (обозначается $X \sim \text{Bin}(n, p)$)** описывает число успехов в серии из $n$ независимых испытаний, каждое из которых имеет две исхода — "успех" (с вероятностью $p$) и "неуспех" (с вероятностью $1 - p$).  
> Функция распределения вероятностей (PMF) биномиальной случайной величины $X$ имеет вид:

$$
 P(X = k) = \binom{n}{k} \, p^k (1 - p)^{n - k}, \quad k = 0, 1, 2, \dots, n.
$$


## 2. Основные параметры биномиального распределения

1. **Математическое ожидание (среднее значение):** $E(X) = np.$  
2. **Дисперсия:** $\mathrm{Var}(X) = np(1 - p).$  
3. **Среднеквадратическое отклонение:** $\sigma_X = \sqrt{np(1 - p)}.$  


## 3. Доказательство $E(X) = np$

### Способ 1. Представление через сумму индикаторов

Рассмотрим $n$ независимых испытаний, каждое из которых даёт исход "успех" (вероятность $p$) или "неуспех" (вероятность $1 - p$). Пусть
$$
X_i = 
\begin{cases}
1, & \text{если в $i$-м испытании произошёл успех},\\
0, & \text{иначе}.
\end{cases}
$$
Тогда биномиальная случайная величина $X$ — это общее число успехов в $n$ испытаниях, то есть
$$
X = X_1 + X_2 + \dots + X_n.
$$
Каждый $X_i$ — это бернуллиевская (Bernoulli) случайная величина с параметром $p$, поэтому
$$
E(X_i) = 1 \cdot p + 0 \cdot (1 - p) = p.
$$
Из свойств математического ожидания (линейность) получаем:
$$
E(X) = E(X_1 + X_2 + \dots + X_n) = E(X_1) + E(X_2) + \dots + E(X_n) = n \cdot p.
$$

### Способ 2. Использование комбинаторного выражения

По определению математического ожидания для дискретной случайной величины:
$$
E(X) = \sum_{k=0}^n k \, P(X = k) = \sum_{k=0}^n k \binom{n}{k} p^k (1-p)^{n-k}.
$$
Обратим внимание, что для $k \ge 1$ верно:
$$
k \binom{n}{k} = n \binom{n-1}{k-1}.
$$
Тогда
$$
E(X) = \sum_{k=1}^n k \binom{n}{k} p^k (1-p)^{n-k}
      = \sum_{k=1}^n n \binom{n-1}{k-1} p^k (1-p)^{n-k}.
$$
Вынесем $n p$ за скобки и сдвинем индекс $k-1 \to j$:
$$
E(X) = n p \sum_{k=1}^n \binom{n-1}{k-1} p^{k-1} (1-p)^{n-k}
      = n p \sum_{j=0}^{n-1} \binom{n-1}{j} p^j (1-p)^{(n-1)-j}.
$$
А эта сумма равна $(p + (1-p))^{n-1} = 1^{n-1} = 1$. Следовательно,
$$
E(X) = n p.
$$

Таким образом, в обоих подходах результат один и тот же: **математическое ожидание биномиальной случайной величины $X \sim \text{Bin}(n, p)$ равно $np$.**



